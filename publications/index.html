<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | YiTao Zhu </title> <meta name="author" content="YiTao Zhu"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%92&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yitao-zhu.github.io//publications/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">YiTao</span> Zhu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Curriculum Vitae </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI (oral)</abbr> <figure> <picture> <img src="/assets/img/publication_preview/muc.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="muc.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhu2025muc" class="col-sm-8"> <div class="title" style="font-size: 22px; font-family: 'Times New Roman', Times, serif;">MUC: Mixture of uncalibrated cameras for robust 3d human body reconstruction</div> <div class="author" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> <em style="color: #2798ba; font-weight: bold;">Yitao Zhu<sup>*</sup></em>, Sheng Wang<sup>*</sup>, <a href="https://xum007.github.io/" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Mengjie Xu</a>, Zixu Zhuang, Zhixin Wang, Kaidong Wang, Han Zhang, and <a href="https://qianwang.space/" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Qian Wang<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* Co-first author&lt;br&gt;† Corresponding author"> </i> </div> <div class="periodical" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> <span style="font-style: italic;"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em> </span> , <span style="font-style: normal;"> 2025 </span> </div> <div class="periodical" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2403.05055v1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/AbsterZhu/MUC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=UR1eH9cAAAAJ&amp;citation_for_view=UR1eH9cAAAAJ:9yKSN-GCB0IC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Multiple cameras can provide comprehensive multi-view video coverage of a person. Fusing this multi-view data is crucial for tasks like behavioral analysis, although it traditionally requires camera calibration, a process that is often complex. Moreover, previous studies have overlooked the challenges posed by self-occlusion under multiple views and the continuity of human body shape estimation. In this study, we introduce a method to reconstruct the 3D human body from multiple uncalibrated camera views. Initially, we utilize a pre-trained human body encoder to process each camera view individually, enabling the reconstruction of human body models and parameters for each view along with predicted camera positions. Rather than merely averaging the models across views, we develop a neural network trained to assign weights to individual views for all human body joints, based on the estimated distribution of joint distances from each camera. Additionally, we focus on the mesh surface of the human body for dynamic fusion, allowing for the seamless integration of facial expressions and body shape into a unified human body model. Our method has shown excellent performance in reconstructing the human body on two public datasets, advancing beyond previous work from the SMPL model to the SMPL-X model. This extension incorporates more complex hand poses and facial expressions, enhancing the detail and accuracy of the reconstructions. Crucially, it supports the flexible ad-hoc deployment of any number of cameras, offering significant potential for various applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR (highlight)</abbr> <figure> <picture> <img src="/assets/img/publication_preview/mitracker.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mitracker.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xu2025mitracker" class="col-sm-8"> <div class="title" style="font-size: 22px; font-family: 'Times New Roman', Times, serif;">MITracker: Multi-View Integration for Visual Object Tracking</div> <div class="author" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> <a href="https://xum007.github.io/" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Mengjie Xu<sup>*</sup></a>, <em style="color: #2798ba; font-weight: bold;">Yitao Zhu<sup>*</sup></em>, Haotian Jiang, Jiaming Li, Zhenrong Shen, Sheng Wang, Haolin Huang, Xinyu Wang, Qing Yang, Han Zhang, and <a href="https://qianwang.space/" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Qian Wang<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* Co-first author&lt;br&gt;† Corresponding author"> </i> </div> <div class="periodical" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> <span style="font-style: italic;"> <em>IEEE/CVF Computer Vision and Pattern Recognition Conference</em> </span> , <span style="font-style: normal;"> 2025 </span> </div> <div class="periodical" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.20111" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://mii-laboratory.github.io/MITracker/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/XuM007/MITracker" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=UR1eH9cAAAAJ&amp;citation_for_view=UR1eH9cAAAAJ:qjMakFHDy7sC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Multi-view object tracking (MVOT) offers promising solutions to challenges such as occlusion and target loss, which are common in traditional single-view tracking. However, progress has been limited by the lack of comprehensive multi-view datasets and effective cross-view integration methods. To overcome these limitations, we compiled a Multi-View object Tracking (MVTrack) dataset of 234K high-quality annotated frames featuring 27 distinct objects across various scenes. In conjunction with this dataset, we introduce a novel MVOT method, Multi-View Integration Tracker (MITracker), to efficiently integrate multi-view object features and provide stable tracking outcomes. MITracker can track any object in video frames of arbitrary length from arbitrary viewpoints. The key advancements of our method over traditional single-view approaches come from two aspects: (1) MITracker transforms 2D image features into a 3D feature volume and compresses it into a bird’s eye view (BEV) plane, facilitating inter-view information fusion; (2) we propose an attention mechanism that leverages geometric information from fused 3D feature volume to refine the tracking results at each view. MITracker outperforms existing methods on the MVTrack and GMTD datasets, achieving state-of-the-art performance. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI 2025 (EA)</abbr> <figure> <picture> <img src="/assets/img/publication_preview/medlego.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="medlego.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhu2025med" class="col-sm-8"> <div class="title" style="font-size: 22px; font-family: 'Times New Roman', Times, serif;">Med-LEGO: Editing and Adapting toward Generalist Medical Image Diagnosis</div> <div class="author" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> <em style="color: #2798ba; font-weight: bold;">Yitao Zhu</em>, Yuan Yin, Jiaming Li, <a href="https://xum007.github.io/" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Mengjie Xu</a>, Zihao Zhao, Honglin Xiong, Sheng Wang, and <a href="https://qianwang.space/" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Qian Wang<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† Corresponding author"> </i> </div> <div class="periodical" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> <span style="font-style: italic;"> <em>arXiv preprint arXiv:2503.01164</em> </span> , <span style="font-style: normal;"> 2025 </span> </div> <div class="periodical" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2503.01164" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=UR1eH9cAAAAJ&amp;citation_for_view=UR1eH9cAAAAJ:UeHWp8X0CEIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The adoption of visual foundation models has become a common practice in computer-aided diagnosis (CAD). While these foundation models provide a viable solution for creating generalist medical AI, privacy concerns make it difficult to pre-train or continuously update such models across multiple domains and datasets, leading many studies to focus on specialist models. To address this challenge, we propose Med-LEGO, a training-free framework that enables the seamless integration or updating of a generalist CAD model by combining multiple specialist models, similar to assembling LEGO bricks. Med-LEGO enhances LoRA (low-rank adaptation) by incorporating singular value decomposition (SVD) to efficiently capture the domain expertise of each specialist model with minimal additional parameters. By combining these adapted weights through simple operations, Med-LEGO allows for the easy integration or modification of specific diagnostic capabilities without the need for original data or retraining. Finally, the combined model can be further adapted to new diagnostic tasks, making it a versatile generalist model. Our extensive experiments demonstrate that Med-LEGO outperforms existing methods in both cross-domain and in-domain medical tasks while using only 0.18% of full model parameters. These merged models show better convergence and generalization to new tasks, providing an effective path toward generalist medical AI.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TMI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/chatcad.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="chatcad.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhao2024chatcad+" class="col-sm-8"> <div class="title" style="font-size: 22px; font-family: 'Times New Roman', Times, serif;">Chatcad+: Towards a universal and reliable interactive cad using llms</div> <div class="author" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> Zihao Zhao<sup>*</sup>, Sheng Wang<sup>*</sup>, Jinchen Gu<sup>*</sup>, <em style="color: #2798ba; font-weight: bold;">Yitao Zhu<sup>*</sup></em>, Lanzhuju Mei, Zixu Zhuang, <a href="https://shanghaitech-impact.github.io/" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Zhiming Cui</a>, <a href="https://qianwang.space/" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Qian Wang</a>, and <a href="https://idea.bme.shanghaitech.edu.cn/main.htm" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Dinggang Shen<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* Co-first author&lt;br&gt;† Corresponding author"> </i> </div> <div class="periodical" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> <span style="font-style: italic;"> <em>IEEE Transactions on Medical Imaging</em> </span> , <span style="font-style: normal;"> 2024 </span> </div> <div class="periodical" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.15964" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/zhaozh10/ChatCAD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=UR1eH9cAAAAJ&amp;citation_for_view=UR1eH9cAAAAJ:u-x6o8ySG0sC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The integration of Computer-Aided Diagnosis (CAD) with Large Language Models (LLMs) presents a promising frontier in clinical applications, notably in automating diagnostic processes akin to those performed by radiologists and providing consultations similar to a virtual family doctor. Despite the promising potential of this integration, current works face at least two limitations: (1) From the perspective of a radiologist, existing studies typically have a restricted scope of applicable imaging domains, failing to meet the diagnostic needs of different patients. Also, the insufficient diagnostic capability of LLMs further undermine the quality and reliability of the generated medical reports. (2) Current LLMs lack the requisite depth in medical expertise, rendering them less effective as virtual family doctors due to the potential unreliability of the advice provided during patient consultations. To address these limitations, we introduce ChatCAD+, to be universal and reliable. Specifically, it is featured by two main modules: (1) Reliable Report Generation and (2) Reliable Interaction. The Reliable Report Generation module is capable of interpreting medical images from diverse domains and generate high-quality medical reports via our proposed hierarchical in-context learning. Concurrently, the interaction module leverages up-to-date information from reputable medical websites to provide reliable medical advice. Together, these designed modules synergize to closely align with the expertise of human medical professionals, offering enhanced consistency and reliability for interpretation and advice.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ISBI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/inter.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="inter.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2024inter" class="col-sm-8"> <div class="title" style="font-size: 22px; font-family: 'Times New Roman', Times, serif;">Inter-slice super-resolution of magnetic resonance images by pre-training and self-supervised fine-tuning</div> <div class="author" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> Xin Wang<sup>*</sup>, Zhiyun Song<sup>*</sup>, <em style="color: #2798ba; font-weight: bold;">Yitao Zhu</em>, Sheng Wang, Lichi Zhang, <a href="https://idea.bme.shanghaitech.edu.cn/main.htm" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Dinggang Shen</a>, and <a href="https://qianwang.space/" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Qian Wang<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* Co-first author&lt;br&gt;† Corresponding author"> </i> </div> <div class="periodical" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> <span style="font-style: italic;"> <em>In 2024 IEEE International Symposium on Biomedical Imaging (ISBI)</em> </span> , <span style="font-style: normal;"> 2024 </span> </div> <div class="periodical" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2406.05974" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=UR1eH9cAAAAJ&amp;citation_for_view=UR1eH9cAAAAJ:2osOgNQ5qMEC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>In clinical practice, 2D magnetic resonance (MR) sequences are widely adopted. While individual 2D slices can be stacked to form a 3D volume, the relatively large slice spacing can pose challenges for both image visualization and subsequent analysis tasks, which often require isotropic voxel spacing. To reduce slice spacing, deep-learning-based super-resolution techniques are widely investigated. However, most current solutions require a substantial number of paired high-resolution and low-resolution images for supervised training, which are typically unavailable in real-world scenarios. In this work, we propose a self-supervised super-resolution framework for inter-slice super-resolution of MR images. Our framework is first featured by pre-training on video dataset, as temporal correlation of videos is found beneficial for modeling the spatial relation among MR slices. Then, we use public high-quality MR dataset to fine-tune our pre-trained model, for enhancing awareness of our model to medical data. Finally, given a target dataset at hand, we utilize self-supervised fine-tuning to further ensure our model works well with user-specific super-resolution tasks. The proposed method demonstrates superior performance compared to other self-supervised methods and also holds the potential to benefit various downstream applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ISBI (oral)</abbr> <figure> <picture> <img src="/assets/img/publication_preview/melo.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="melo.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhu2024melo" class="col-sm-8"> <div class="title" style="font-size: 22px; font-family: 'Times New Roman', Times, serif;">Melo: Low-rank adaptation is better than fine-tuning for medical image diagnosis</div> <div class="author" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> <em style="color: #2798ba; font-weight: bold;">Yitao Zhu</em>, Zhenrong Shen, Zihao Zhao, Sheng Wang, Xin Wang, <a href="https://hsiangyuzhao.github.io/" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Xiangyu Zhao</a>, <a href="https://idea.bme.shanghaitech.edu.cn/main.htm" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Dinggang Shen</a>, and <a href="https://qianwang.space/" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Qian Wang<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† Corresponding author"> </i> </div> <div class="periodical" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> <span style="font-style: italic;"> <em>In 2024 IEEE International Symposium on Biomedical Imaging (ISBI)</em> </span> , <span style="font-style: normal;"> 2024 </span> </div> <div class="periodical" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2311.08236" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/JamesQFreeman/LoRA-ViT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=UR1eH9cAAAAJ&amp;citation_for_view=UR1eH9cAAAAJ:d1gkVwhDpl0C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The common practice in developing computer-aided diagnosis (CAD) models based on transformer architectures usually involves fine-tuning from ImageNet pre-trained weights. However, with recent advances in large-scale pre-training and the practice of scaling laws, Vision Transformers (ViT) have become much larger and less accessible to medical imaging communities. Additionally, in real-world scenarios, the deployments of multiple CAD models can be troublesome due to problems such as limited storage space and time-consuming model switching. To address these challenges, we propose a new method MeLo (Medical image Low-rank adaptation), which enables the development of a single CAD model for multiple clinical tasks in a lightweight manner. It adopts low-rank adaptation instead of resource-demanding fine-tuning. By fixing the weight of ViT models and only adding small low-rank plug-ins, we achieve competitive results on various diagnosis tasks across different imaging modalities using only a few trainable parameters. Specifically, our proposed method achieves comparable performance to fully fine-tuned ViT models on four distinct medical imaging datasets using about 0.17% trainable parameters. Moreover, MeLo adds only about 0.5MB of storage space and allows for extremely fast model switching in deployment and inference.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/doctorglm.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="doctorglm.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xiong2023doctorglm" class="col-sm-8"> <div class="title" style="font-size: 22px; font-family: 'Times New Roman', Times, serif;">Doctorglm: Fine-tuning your chinese doctor is not a herculean task</div> <div class="author" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> Honglin Xiong<sup>*</sup>, Sheng Wang<sup>*</sup>, <em style="color: #2798ba; font-weight: bold;">Yitao Zhu<sup>*</sup></em>, Zihao Zhao<sup>*</sup>, Yuxiao Liu, Linlin Huang, <a href="https://qianwang.space/" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Qian Wang</a>, and <a href="https://idea.bme.shanghaitech.edu.cn/main.htm" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Dinggang Shen<sup>†</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* Co-first author&lt;br&gt;† Corresponding author"> </i> </div> <div class="periodical" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> <span style="font-style: italic;"> <em>arXiv preprint arXiv:2304.01097</em> </span> , <span style="font-style: normal;"> 2023 </span> </div> <div class="periodical" style="font-size: 16px; font-family: 'Times New Roman', Times, serif;"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2304.01097" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/xionghonglin/DoctorGLM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=UR1eH9cAAAAJ&amp;citation_for_view=UR1eH9cAAAAJ:u5HHmVD_uO8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-N/A-4285F4?logo=googlescholar&amp;labelColor=beige" alt="N/A Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The recent progress of large language models (LLMs), including ChatGPT and GPT-4, in comprehending and responding to human instructions has been remarkable. Nevertheless, these models typically perform better in English and have not been explicitly trained for the medical domain, resulting in suboptimal precision in diagnoses, drug recommendations, and other medical advice. Additionally, training and deploying a dialogue model is still believed to be impossible for hospitals, hindering the promotion of LLMs. To tackle these challenges, we have collected databases of medical dialogues in Chinese with ChatGPT’s help and adopted several techniques to train an easy-deploy LLM. Remarkably, we were able to fine-tune the ChatGLM-6B on a single A100 80G in 13 hours, which means having a healthcare-purpose LLM can be very affordable. DoctorGLM is currently an early-stage engineering attempt and contain various mistakes. </p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 YiTao Zhu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>